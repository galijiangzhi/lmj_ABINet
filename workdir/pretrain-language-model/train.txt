ModelConfig(
	(0): dataset_case_sensitive = False
	(1): dataset_charset_path = data/charset_36.txt
	(2): dataset_data_aug = True
	(3): dataset_eval_case_sensitive = False
	(4): dataset_image_height = 32
	(5): dataset_image_width = 128
	(6): dataset_max_length = 25
	(7): dataset_multiscales = False
	(8): dataset_num_workers = 4
	(9): dataset_one_hot_y = True
	(10): dataset_pin_memory = True
	(11): dataset_smooth_factor = 0.1
	(12): dataset_smooth_label = False
	(13): dataset_test_batch_size = 1000
	(14): dataset_test_roots = ['data/language/lang_test.csv']
	(15): dataset_train_batch_size = 1000
	(16): dataset_train_roots = ['data/language/lang_train.csv']
	(17): dataset_use_sm = False
	(18): global_name = pretrain-language-model
	(19): global_phase = train
	(20): global_seed = None
	(21): global_stage = pretrain-language
	(22): global_workdir = workdir/pretrain-language-model
	(23): model_checkpoint = None
	(24): model_language_loss_weight = 1.0
	(25): model_language_num_layers = 4
	(26): model_language_use_self_attn = False
	(27): model_name = modules.model_language.BCNLanguage
	(28): model_strict = True
	(29): optimizer_args_betas = (0.9, 0.999)
	(30): optimizer_bn_wd = False
	(31): optimizer_clip_grad = 20
	(32): optimizer_lr = 0.0001
	(33): optimizer_scheduler_gamma = 0.1
	(34): optimizer_scheduler_periods = [70, 10]
	(35): optimizer_true_wd = False
	(36): optimizer_type = Adam
	(37): optimizer_wd = 0.0
	(38): training_epochs = 80
	(39): training_eval_iters = 3000
	(40): training_save_iters = 3000
	(41): training_show_iters = 20
	(42): training_start_iters = 0
	(43): training_stats_iters = 100000
)
Construct dataset.
3443 training items found.
3443 valid items found.
Construct model.
BCNLanguage(
  (proj): Linear(in_features=69, out_features=512, bias=False)
  (token_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (model): TransformerDecoder(
    (layers): ModuleList(
      (0-3): 4 x TransformerDecoderLayer(
        (multihead_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (cls): Linear(in_features=512, out_features=69, bias=True)
)
Construct learner.
Start training.
epoch 6 iter 20: loss = 2.5903,  smooth loss = 2.9328
epoch 13 iter 40: loss = 2.3633,  smooth loss = 2.6630
epoch 20 iter 60: loss = 2.1432,  smooth loss = 2.4629
epoch 26 iter 80: loss = 2.0938,  smooth loss = 2.3237
epoch 33 iter 100: loss = 2.0169,  smooth loss = 2.2182
epoch 40 iter 120: loss = 1.9087,  smooth loss = 2.1242
epoch 46 iter 140: loss = 1.8344,  smooth loss = 2.0346
epoch 53 iter 160: loss = 1.8077,  smooth loss = 1.9558
epoch 60 iter 180: loss = 1.7458,  smooth loss = 1.8884
epoch 66 iter 200: loss = 1.7050,  smooth loss = 1.8299
epoch 73 iter 220: loss = 1.6930,  smooth loss = 1.7800
