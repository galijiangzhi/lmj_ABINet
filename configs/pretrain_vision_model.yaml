global:
  name: pretrain-vision-model
  phase: train
  stage: pretrain-vision
  workdir: workdir
  seed: ~
 
dataset:
  train: {
    roots: [
      'data/vision/xunlian_train',
            'data/vision/lmdb_train',
            'data/vision/lmj_train',
    ],
    batch_size: 20
  }
  test: {
    roots: [
      'data/vision/ceshi_test',
            'data/vision/lmdb_test',
            'data/vision/lmj_test',
    ],
    batch_size: 20
  }
  data_aug: True
  multiscales: False
  num_workers: 14

training:
  epochs: 20
  show_iters: 50
  eval_iters: 20
  save_iters: 20

optimizer:
  type: Adam
  true_wd: False
  wd: 0.0
  bn_wd: False
  clip_grad: 20
  lr: 0.0001
  args: {
    betas: !!python/tuple [0.9, 0.999], # for default Adam 
  }
  scheduler: {
    periods: [6, 2],
    gamma: 0.1,
  }
dataset_charset_path: data/vision/tibetan_chars.txt
model:
  name: 'modules.model_vision.BaseVision'
  checkpoint: ~
  vision: {
    loss_weight: 1.,
    attention: 'position',
    backbone: 'transformer',
    backbone_ln: 3,
  }
